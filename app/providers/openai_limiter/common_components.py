"""å…¬å…±åŸºç¡€ç»„ä»¶ï¼šé€šç”¨é™æµä¸ç¼“å­˜åŸºç±»

æä¾›ï¼š
 - BaseRateLimiterï¼šæ—¶é—´çª—å£å†…è¯·æ±‚æ•°ä¸ token æ•°åŒæŒ‡æ ‡é™æµ
 - BaseResponseCacheï¼šTTL + å®¹é‡é™åˆ¶ + å¯é€‰æŒä¹…åŒ– çš„å¼‚æ­¥ç¼“å­˜
 - ModelBasedManagerï¼šæŒ‰æ¨¡å‹ååˆ†ç¦»é™æµå™¨å’Œç¼“å­˜çš„ç®¡ç†å™¨

"""
from __future__ import annotations

import asyncio
import json
import time
from dataclasses import dataclass
from typing import Any, Dict, Optional, Protocol, List, TypeVar, Callable


# ----------------------------- é™æµ ----------------------------- #
class HasMaxConfig(Protocol):  # ä¸ºç±»å‹æç¤ºçº¦æŸ config
    max_requests_per_minute: int
    max_tokens_per_minute: int


class BaseRateLimiter:
    """é€šç”¨é™æµå™¨ï¼ˆæ»‘åŠ¨ 60s çª—å£ï¼‰

    å­ç±»åªéœ€æä¾› labelï¼ˆæ‰“å°ç”¨ï¼‰ã€‚
    """

    label: str = "Generic"

    def __init__(self, config: HasMaxConfig):
        self.config = config
        self.request_timestamps: List[float] = []
        self.token_usage: List[tuple[float, int]] = []
        self._lock = asyncio.Lock()

    async def wait_if_needed(self, estimated_tokens: int = 0):
        async with self._lock:
            now = time.time()
            minute_ago = now - 60

            # æ¸…ç†è¿‡æœŸè®°å½•
            self.request_timestamps = [t for t in self.request_timestamps if t > minute_ago]
            self.token_usage = [(t, tk) for t, tk in self.token_usage if t > minute_ago]

            # è¯·æ±‚æ•°é™åˆ¶
            if len(self.request_timestamps) >= self.config.max_requests_per_minute:
                sleep_time = 60 - (now - self.request_timestamps[0])
                if sleep_time > 0:
                    print(f"â³ è¾¾åˆ° {self.label} è¯·æ±‚é™åˆ¶ ({self.config.max_requests_per_minute}/åˆ†é’Ÿ)ï¼Œç­‰å¾… {sleep_time:.1f} ç§’...")
                    await asyncio.sleep(sleep_time)

            # token é™åˆ¶
            current_tokens = sum(tokens for _, tokens in self.token_usage)
            if current_tokens + estimated_tokens >= self.config.max_tokens_per_minute:
                sleep_time = 60 - (now - self.token_usage[0][0]) if self.token_usage else 60
                if sleep_time > 0:
                    print(f"â³ è¾¾åˆ° {self.label} Token é™åˆ¶ ({self.config.max_tokens_per_minute}/åˆ†é’Ÿ)ï¼Œç­‰å¾… {sleep_time:.1f} ç§’...")
                    await asyncio.sleep(sleep_time)

            # è®°å½•å½“å‰è¯·æ±‚ï¼ˆå…ˆè®°å½•ä¼°ç®—å€¼ï¼Œåç»­å¯ updateï¼‰
            now2 = time.time()
            self.request_timestamps.append(now2)
            self.token_usage.append((now2, estimated_tokens))

    def update_actual_tokens(self, actual_tokens: int):
        if self.token_usage:
            last_time, _ = self.token_usage[-1]
            self.token_usage[-1] = (last_time, actual_tokens)

    def get_stats(self) -> Dict[str, Any]:
        now = time.time()
        minute_ago = now - 60
        active_requests = len([t for t in self.request_timestamps if t > minute_ago])
        active_tokens = sum(tokens for t, tokens in self.token_usage if t > minute_ago)
        return {
            "current_requests": active_requests,
            "current_tokens": active_tokens,
            "max_requests_per_minute": self.config.max_requests_per_minute,
            "max_tokens_per_minute": self.config.max_tokens_per_minute,
            "label": self.label,
        }


# ----------------------------- ç¼“å­˜ ----------------------------- #

@dataclass
class BaseCacheConfig:
    enabled: bool = True
    max_size: int = 1000
    ttl_seconds: int = 3600
    persist_to_file: bool = False
    cache_file_path: str = "generic_cache.json"


class BaseResponseCache:
    """é€šç”¨å“åº”ç¼“å­˜åŸºç±»

    ä¾èµ–å­ç±»å®ç° _generate_keyã€‚
    å­˜å‚¨ç»“æ„ï¼š{ key: { "timestamp": float, ...payload... } }
    """

    label: str = "Cache"

    def __init__(self, config: BaseCacheConfig):
        self.config = config
        self.cache: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
        if self.config.persist_to_file:
            self._load_cache()

    # --- ç•™ç»™å­ç±»çš„æ¥å£ ---
    def _generate_key(self, *args, **kwargs) -> str:  # pragma: no cover - å­ç±»å®ç°
        raise NotImplementedError

    # --- åŸºç¡€å®ç° ---
    def _load_cache(self):
        try:
            with open(self.config.cache_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                now = time.time()
                self.cache = {
                    k: v for k, v in data.items()
                    if now - v.get("timestamp", 0) < self.config.ttl_seconds
                }
        except (FileNotFoundError, json.JSONDecodeError):
            self.cache = {}

    def _save_cache(self):
        if not self.config.persist_to_file:
            print("âš ï¸  æœªå¯ç”¨æŒä¹…åŒ–ï¼Œè·³è¿‡ä¿å­˜ç¼“å­˜ã€‚")
            return
        try:
            with open(self.config.cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
            # æ‰“å°ä¸‹ç›®å‰æœ‰å¤šå°‘ä¸ªç¼“å­˜
            print(f"ğŸ’¾ å·²ä¿å­˜ {self.label} ç¼“å­˜ï¼Œå…± {len(self.cache)} æ¡ã€‚")
        except Exception as e:  
            print(f"âš ï¸  ä¿å­˜ {self.label} ç¼“å­˜å¤±è´¥: {e}")
    

    async def get(self, key: str) -> Optional[Dict[str, Any]]:
        async with self._lock:
            if key not in self.cache:
                return None
            data = self.cache[key]
            if time.time() - data.get("timestamp", 0) > self.config.ttl_seconds:
                del self.cache[key]
                self._save_cache()
                return None
            print(f"ğŸ¯ {self.label} ç¼“å­˜å‘½ä¸­: {key[:16]}...")
            return data

    async def set(self, key: str, value: Dict[str, Any]):
        async with self._lock:
            if len(self.cache) >= self.config.max_size:
                # ç§»é™¤æœ€æ—§
                oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k].get("timestamp", 0))
                del self.cache[oldest_key]
            value["timestamp"] = time.time()
            self.cache[key] = value
            if self.config.persist_to_file:
                self._save_cache()
            print(f"ğŸ’¾ {self.label} ç¼“å­˜å­˜å‚¨: {key[:16]}...")

    def clear(self):
        self.cache.clear()
        if self.config.persist_to_file:
            try:
                import os
                if os.path.exists(self.config.cache_file_path):
                    os.remove(self.config.cache_file_path)
            except Exception as e:  # pragma: no cover
                print(f"âš ï¸  æ¸…ç©º {self.label} ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    def get_stats(self) -> Dict[str, Any]:
        now = time.time()
        valid_entries = sum(
            1 for v in self.cache.values() if now - v.get("timestamp", 0) < self.config.ttl_seconds
        )
        return {
            "total_entries": len(self.cache),
            "valid_entries": valid_entries,
            "max_size": self.config.max_size,
            "ttl_seconds": self.config.ttl_seconds,
            "persist_to_file": self.config.persist_to_file,
            "label": self.label,
        }


__all__ = [
    "BaseRateLimiter",
    "BaseResponseCache", 
    "BaseCacheConfig",
    "ModelBasedLimiterManager",
    "ModelBasedCacheManager",
]


# ----------------------------- æŒ‰æ¨¡å‹åˆ†ç¦»ç®¡ç†å™¨ ----------------------------- #

T = TypeVar('T')
ConfigType = TypeVar('ConfigType')

class ModelBasedLimiterManager:
    """æŒ‰æ¨¡å‹åç®¡ç†ç‹¬ç«‹é™æµå™¨çš„ç®¡ç†å™¨
    
    ç”¨æ³•ï¼š
    manager = ModelBasedLimiterManager(EmbeddingLimiter, default_config)
    limiter = manager.get_limiter("text-embedding-3-small", model_specific_config)
    """
    
    def __init__(self, 
                 limiter_class: type[BaseRateLimiter],
                 default_config: Any,
                 model_configs: Dict[str, Any] = None):
        self.limiter_class = limiter_class
        self.default_config = default_config
        self.model_configs = model_configs or {}
        self.limiters: Dict[str, BaseRateLimiter] = {}
    
    def get_limiter(self, model: str) -> BaseRateLimiter:
        """è·å–æŒ‡å®šæ¨¡å‹çš„é™æµå™¨ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if model not in self.limiters:
            config = self.model_configs.get(model, self.default_config)
            self.limiters[model] = self.limiter_class(config)
        return self.limiters[model]
    
    def get_all_stats(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ¨¡å‹çš„é™æµç»Ÿè®¡"""
        return {model: limiter.get_stats() for model, limiter in self.limiters.items()}


class ModelBasedCacheManager:
    """æŒ‰æ¨¡å‹åç®¡ç†ç‹¬ç«‹ç¼“å­˜çš„ç®¡ç†å™¨
    
    ç”¨æ³•ï¼š
    manager = ModelBasedCacheManager(EmbeddingResponseCache, default_config)
    cache = manager.get_cache("text-embedding-3-small", model_specific_config)
    """
    
    def __init__(self,
                 cache_class: type[BaseResponseCache], 
                 default_config: Any,
                 model_configs: Dict[str, Any] = None):
        self.cache_class = cache_class
        self.default_config = default_config
        self.model_configs = model_configs or {}
        self.caches: Dict[str, BaseResponseCache] = {}
    
    def get_cache(self, model: str) -> BaseResponseCache:
        """è·å–æŒ‡å®šæ¨¡å‹çš„ç¼“å­˜ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if model not in self.caches:
            config = self.model_configs.get(model, self.default_config)
            # ä¸ºæ¯ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„ç¼“å­˜æ–‡ä»¶è·¯å¾„
            if hasattr(config, 'cache_file_path') and config.persist_to_file:
                # åœ¨æ–‡ä»¶åä¸­æ’å…¥æ¨¡å‹åé¿å…å†²çª
                original_path = config.cache_file_path
                name, ext = original_path.rsplit('.', 1) if '.' in original_path else (original_path, 'json')
                model_safe = model.replace('/', '_').replace(':', '_')  # å¤„ç†æ¨¡å‹åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                config.cache_file_path = f"{name}_{model_safe}.{ext}"
            
            self.caches[model] = self.cache_class(config)
        return self.caches[model]
    
    def get_all_stats(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ¨¡å‹çš„ç¼“å­˜ç»Ÿè®¡"""
        return {model: cache.get_stats() for model, cache in self.caches.items()}
    
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰æ¨¡å‹çš„ç¼“å­˜"""
        for cache in self.caches.values():
            cache.clear()
        self.caches.clear()
